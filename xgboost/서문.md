
Github
+ https://github.com/rickiepark/handson-gb

# 서문

### 1. 다루는 내용들

| Chapter   | 내용                                                         |      |
| --------- | ------------------------------------------------------------ | ---- |
| **Part1** | **배깅과 부스팅**                                            |      |
| 1장       | 선형회귀, 로지스틱회귀, XGBoost와 비교, Pandas               |      |
| 2장       | XGBoost의 결정 트리 하이퍼파라미터, 분산과 편향, 과대적합    |      |
| 3장       | 배깅, 랜덤 포레스트, `n_estimators`, `subsample` 파라미터    |      |
| 4장       | 기본적인 부스팅, Gradient Boosting, XGBoost 속도, `eta`      |      |
| **Part2** | **XGBoost**                                                  |      |
| 5장       | XGBoost의 수학 이론, XGBoost의 역할, XGBoost의 기본 파라미터 |      |
| 6장       | XGBoost의 핵심 파라미터, 트리 기반 앙상블의 하이퍼파라미터 요약, GridSearch |      |
| 7장       | 사례 연구, 오차 행렬, 분류 리포트, 성능 지표, `scale_pos_weight` (불균형 자료 파라미터) |      |
| **Part3** | **고급 XGBoost**                                             |      |
| 8장       | gbtree, dart, gblinear 등의 XGBoost의 모든 부스터, XGBoost로 회귀와 분류 |      |
| 9장       | XGBoost 팁과 기법 : 특성 공학, 상관관계 낮은 앙상블, 스태킹  |      |
| 10장      | XGBoost를 위한 데이터 변환, 머신러닝 파이프라인              |      |
| **부록**  |                                                              |      |
| A         | LightGBM, 히스토그램 기반 Gradient Boosting, CatBoost        |      |







# Ch7

## 7.3. 불균형자료 리샘플링

### 7.3.3. 오버샘플링

train과 test로 split하기 전에 오버샘플링하면, 오버샘플링한 똑같은 자료가 train과 test 모두에 들어가기 때문에, split하고 난 이후에 오버샘플링 해야함





## 7.4. XGBClassifier 튜닝 for 불균형자료

### 7.4.1. `scale_pos_weight`

사용법

+ 음성 자료의 수가 양성 자료의 몇 배인지 입력

