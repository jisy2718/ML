{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82521565",
   "metadata": {},
   "source": [
    "# Ch05. XGBoost\n",
    "**구성**\n",
    "1. XGBoost가 트리 앙상블 알고리즘을 향상시킨 이론(XGBoost의 속도 향상, 누락된 값 처리 방법, 규제 매개변수의 수학이론)\n",
    "    + XGBoost 구조\n",
    "    + XGBoost 매개변수 분석\n",
    "2. 힉스 보손 캐글 대회\n",
    "    + XGBoost 모델 만들기(분류, 회귀)\n",
    "    + 대회 사례 연구"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d810a11",
   "metadata": {},
   "source": [
    "## 5.1. XGBoost 구조\n",
    "XGBoost는 Gradient 부스팅을 크게 업그레이드한 모델\n",
    "\n",
    "### 5.1.1. 역사\n",
    "+ **결정트리**의 경우 가지치기 없이 모든 리프 노드가 순수노드가 되는 경우, 새로운 데이터에 대한 일반화 성능이 떨어집니다.\n",
    "+ **앙상블 방법**은 **배깅**과 **부스팅**을 통해 많은 결정 트리를 연결하기 때문에 더 효과적\n",
    "    + 앙상블 중에 선두 알고리즘은 Gradient Boosting\n",
    "+ 워싱턴 대학의 티엔치 첸은 Gradient Boosting의 일관성, 성능, 뛰어난 결과를 더욱 향상 시킨, **익스트림 그레디언트 부스팅 : XGBoost**를 고안함\n",
    "    + 내장된 규제와 속도 향상이 포함됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbdfeda",
   "metadata": {},
   "source": [
    "### 5.1.2. 주요 기능\n",
    "**XGBoost**는 계산 능력을 극대화한다는 의미로, 이를 위해서는 모델 구축뿐 아니라 디스크 입출력, 압축, 캐싱, cpu에 대한 지식이 필요\n",
    "\n",
    "#### 1) 누락된 값 처리\n",
    "XGBoost는 자체적으로 누락된 값을 처리 가능\n",
    "+ `missing` 매개변수에 값을 지정 가능\n",
    "    + XGBoost는 누락 값을 좌/우측으로 보내는 분할들 중 최선의 결과를 내는 분할을 선택\n",
    "    \n",
    "#### 2) 속도향상\n",
    "XGBoost는 특히 속도에 주안점을 두고 설계됨. 다음과 같은 기능이 속도를 향상시킴\n",
    "+ 근사 분할 탐색 알고리즘\n",
    "+ 희소성 고려 분할 탐색\n",
    "+ 병렬 컴퓨팅\n",
    "+ 캐시 고려 접근\n",
    "+ 블록 압축과 샤딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c1292",
   "metadata": {},
   "source": [
    "#### 2-1) 근사 분할 탐색 알고리즘\n",
    "이는 데이터를 나누는 퍼센트인 분위수를 사용하여 후보 분할을 제안함\n",
    "+ 전역 제안(global proposal) : 동일한 분위수가 전체 훈련에 사용됨\n",
    "+ 지역 제안(local proposal) : 각 분할마다 새로운 분위수를 제안\n",
    "\n",
    "콴타일 스케치 알고리즘(quantile sketch algorithm)은 가중치가 균일한 데이터 셋에서 잘 작동함. \n",
    "+ XGBoost는 가중 콴타일 스케치를 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb099736",
   "metadata": {},
   "source": [
    "#### 2-2) 희소성 고려 분할 탐색\n",
    "분할을 탐색할 때, 희소한 행렬에서 XGBoost가 더 빠르게 동작하게 됨\n",
    "\n",
    "#### 2-3) 병렬컴퓨팅\n",
    "부스팅은 각 트리가 이전 트리의 결과에 의존하기 때문에 병렬 컴퓨팅에 이상적이지 않지만, 병렬화가 가능한 부분이 있음\n",
    "+ XGBoost는 데이터를 블록(block)단위로 정렬하고 압축함\n",
    "+ 블록은 여러 대의 머신이나 외부 메모리에 분산될 수 있음\n",
    "+ 분할 탐색 알고리즘은 블록의 장점을 사용해 분위수 탐색을 빠르게 수행함\n",
    "+ 해당 부분에 병렬 컴퓨팅을 사용하여 모델 구축 과정의 속도를 높일 수 있음\n",
    "\n",
    "\n",
    "#### 2-4) 캐시 고려 접근\n",
    "컴퓨터의 데이터는 캐시와 메인 메모리에 나뉘어 있습니다. 가장 빈번하게 사용되는 캐시는 고속 메모리를 사용합니다. 자주 사용하지 않는 데이터는 저속 메모리에 저장됩니다.\n",
    "\n",
    "XGBoost는 캐시를 고려한 프리페칭(prefetching)을 사용하여, 많은 샘플을 가진 데이터셋의 실행 부하를 50% 절감"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae0a2e8",
   "metadata": {},
   "source": [
    "#### 3) 정확도 향상\n",
    "XGBoost는 자체적으로 **규제**를 추가하여 Gradient Boosting 이상으로 정확도를 높임. 즉 **XGBoost는 Gradient Boosting의 규제 버전**\n",
    "+ XGBoost는 Gradient Boosting과 RandomForest와 달리, 학습하려는 목적함수의 일부로 규제를 포함함\n",
    "+ **규제**는 분산을 줄이고 과대적합을 방지함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf7ca4",
   "metadata": {},
   "source": [
    "## 5.2. XGBoost 파라미터 최적화\n",
    "[공식문서참고 : Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1101ee",
   "metadata": {},
   "source": [
    "### 5.2.1. 학습 목적\n",
    "XGBoost의 목적 함수는 **손실함수**와 **규제항** 두 부분으로 구성되어 있음\n",
    "+ 손실함수\n",
    "    + 회귀 : MSE\n",
    "    + 분류 : 로지스틱 손실\n",
    "+ 규제항\n",
    "    + 과적합을 막기위한 페널티 항\n",
    "    \n",
    "**트리 앙상블과 XGBoost의 차이점**\n",
    "+ XGBoost는 목적함수에 규제항이 추가되어 있다는 점\n",
    "\n",
    "#### XGBoost의 목적함수\n",
    "$$ obj(\\theta) = l(\\theta) + \\Omega(\\theta)$$\n",
    "\n",
    "#### 손실함수\n",
    "\n",
    "\n",
    "#### 규제 함수\n",
    "\n",
    "#### 최종 목적함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6263d10",
   "metadata": {},
   "source": [
    "## 5.3. XGBoost 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11746402",
   "metadata": {},
   "source": [
    "### 5.3.1. 분류모델 - iris\n",
    "\n",
    "[공식문서 : 하이퍼파라미터](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "#### booster\n",
    "> 기본학습기를 의미함\n",
    "+ default='gbtree' : 그레디언트 부스팅 트리 (8장에서 다른 것 이용하는 예)\n",
    "\n",
    "#### objective\n",
    "> 회귀/분류 선택 & loss 선택\n",
    "+ default='reg:squarederror'\n",
    "+ 그 외 옵션은 위의 하이퍼파라미터 공식문서 참고\n",
    "\n",
    "\n",
    "#### max_depth\n",
    "> 트리의 깊이\n",
    "\n",
    "#### learning_rate\n",
    "> 지정된 비율로 각 트리의 가중치를 감소시켜 분산을 억제 함\n",
    "+ eta 라고도 부름\n",
    "\n",
    "#### n_estimators\n",
    "> 부스팅에 포함할 트리의 개수\n",
    "+ n_estimators를 늘리고 learning_rate를 줄이면 성능을 높일 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d86d49da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 데이터 가져오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n",
    "                  columns=iris['feature_names'] + ['target'])\n",
    "\n",
    "\n",
    "# 데이터를 훈련 세트와 테스트 세트로 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], \n",
    "                                                    iris['target'], random_state=222)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c29d2005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "점수: 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "# 2. 모델 구축하기\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "xgb = XGBClassifier(booster='gbtree', objective='multi:softprob', \n",
    "                    max_depth=6, learning_rate=0.1, n_estimators=100, n_jobs=-1)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print('점수: ' + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344edf66",
   "metadata": {},
   "source": [
    "`accuracy_score()` 함수 대신 `score()` 메서드를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6524c3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947368421052632"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556ae5f",
   "metadata": {},
   "source": [
    "XGBoost의 기본 파이썬 API를 사용하는 경우 부스터(Booster) 객체의 `predict()` 메서드는 `multi:softprob`일 때 확률을 반환하고 `multi:softmax`일 때 클래스 레이블을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46cc49ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9493131 , 0.0268956 , 0.02379128],\n",
       "       [0.9493131 , 0.0268956 , 0.02379128],\n",
       "       [0.02378523, 0.02503003, 0.9511848 ],\n",
       "       [0.04650725, 0.7033573 , 0.25013545],\n",
       "       [0.02336924, 0.9532964 , 0.02333434]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 속성들\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2b9a122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9493131 , 0.0268956 , 0.02379128],\n",
       "       [0.9493131 , 0.0268956 , 0.02379128],\n",
       "       [0.02378523, 0.02503003, 0.9511848 ],\n",
       "       [0.04650725, 0.7033573 , 0.25013545],\n",
       "       [0.02336924, 0.9532964 , 0.02333434]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3.1. 확률 반환\n",
    "param = {'objective': 'multi:softprob', 'num_class': 3}\n",
    "bstr = xgb.train(param, dtrain, 10)\n",
    "bstr.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d114ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 2., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3.2. 확률 최대값인 레이블 반환\n",
    "param = {'objective': 'multi:softmax', 'num_class': 3}\n",
    "bstr = xgb.train(param, dtrain, 10)\n",
    "bstr.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8162633",
   "metadata": {},
   "source": [
    "### 5.3.2. 회귀모델 - 당뇨병데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0b40427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: [63.011 59.705 64.538 63.706 64.588]\n",
      "RMSE 평균: 63.109\n"
     ]
    }
   ],
   "source": [
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror', \n",
    "                   max_depth=6, learning_rate=0.1, n_estimators=100, \n",
    "                   n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(xgb, X, y, \n",
    "                         scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# 평가 점수의 제곱근을 계산\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "# RMSE를 출력\n",
    "print('RMSE:', np.round(rmse, 3))\n",
    "print('RMSE 평균: %0.3f' % (rmse.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96bfe6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>442.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>152.133484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>77.093005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>140.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>211.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>346.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  442.000000\n",
       "mean   152.133484\n",
       "std     77.093005\n",
       "min     25.000000\n",
       "25%     87.000000\n",
       "50%    140.500000\n",
       "75%    211.500000\n",
       "max    346.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69d2f9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 63.109는 1 표준편차 이내이므로 괜찮은 결과\n"
     ]
    }
   ],
   "source": [
    "print('RMSE 63.109는 1 표준편차 이내이므로 괜찮은 결과') # 이렇게 해석해도 되나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6dcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
